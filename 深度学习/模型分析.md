- KNN K-Nearest Neighbor K近邻法    
  - 即以与数据集中最近邻的K个样本，以它们的类别作为投票表决，对数据进行分类  
  - KNN中关于超参数hyperparameter K 和 度量距离的选择  
    - K太小容易受噪点的影响，K太大容易产生民主效应，少数服从多数，集中的点群影响力扩大，侵占周围不同类别的分散点  
    - 可以猜想，随着K值刚开始准确率上升，随后下降，应该找得到一个极大值点  
    - K 和度量距离的选取没有统一的标准，可通过试验进行选取  
  - 数据集分布的问题  
    - 数据分类结果受到数据集在特征空间分布的影响  
    - 如果数据集分布的不均匀，或者某类数据缺失较少，将严重影响分类结果    
  - 关于KNN计算时效性能  
    - 不需要进行训练  
    - 数据集的数据分布决定了特征空间的类别分布  
    - 随着数据集的增加，数据特征维度的增加在选取K个参考样本的计算消耗成线性增加  
    - 如果每次分类，都通过先计算距离，选取K个参考样本的方式，进行分类，计算消耗很大  
    - 因为数据分布就已经决定了分类，可以考虑先计算出特征空间的类别分布，在进行分类时  
      直接通过查表的方式即可知道结果  
  - KNN很少使用  
    - 总的来说，KNN效率很低，因为它的计算方式  
    - 通过KNN来判定高维度物体的类别，会产生不直观的效果，虽然度量距离相等，但是实际情况千差万别    
    
- 线性分类器 f(x)=w*x=b  
  - 线性分类器的权重代表了该像素位置的贡献值  
  - 线性分类器是在特征空间中寻找一个超平面用于分割该类别与非该类别的点  
  - 线性分类器归纳场景和目标状态的能力有限，是比较单一的模型，是将不同场景的数据特征混合在一起组成的  
    比如，同是一张马的图片，有可能马头往左或往右，低头或侧身，正对或背对等。但是由于线性分类器的能力有限  
    由于数据集中马的状态和背景状态分类很多的话，线性分类器这一单一模型达不到较好的复杂的分类场景的任务  
    但是神经网络可以，由于神经网络分层提取特征，所以神经网络可以对很多目标状态和场景状态不同的图片进行分类  
  - 对于不符合线性分类器目标状态或场景状态的图片分类效果差  
